{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 3\n",
    "#### Student Name: Prashasti Garg\n",
    "#### Student ID: 31901611\n",
    "\n",
    "Date: 18/02/2021\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.9 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* `pandas` (for data manipulation and interpretation)\n",
    "* `html5lib` (for reading .html file)\n",
    "* `tabula` (for reading .pdf file)\n",
    "* `PyPDF2` (for working with .pdf file)\n",
    "* `json` (for reading .json file)\n",
    "* `xmltodict` (for reading .xml file)\n",
    "* `lxml` (for parsing xml and html file)\n",
    "* `xml.etree.ElementTree` (for implementing a simple and efficient for parsing and creating xml data)\n",
    "* `math` (for solving mathematical functions)\n",
    "* `shapefile` (for providing reading and writing support to the shapefile)\n",
    "* `shapely.geometry` (for working with shape file)\n",
    "* `datetime` (for working with date and time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 01 -  Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Integration is the process of combining the data from all different sources in a single, unified view. In Task 01, we have been provided seven files. These file are in .json, .xml, .html, .xlsx, .pdf and two in .zip format. The task is to extract data from these seven file and to create a .csv file which would have 21 columns. These 21 columns are:\n",
    "1. **Property_id**: A unique id for the property\n",
    "2.**lat**: The property latitude. \n",
    "3.**lng**: The property longitude.\n",
    "4.**addr_street**: The property address\n",
    "5.**suburb**: The property suburb. Default value: “not available”.\n",
    "6.**price**: The property price.\n",
    "7.**property_type**: The type of the property.\n",
    "8.**year**: Year of sold.\n",
    "9.**bedrooms**: Number of bedrooms.\n",
    "10.**bathrooms**: Number of bathrooms. \n",
    "11.**parking_space**: The number of parking space of the property.\n",
    "12.**Shopping_center_id**: The closest shopping centre to the property. Default value: “not available”.\n",
    "13.**Distance_to_sc**: The Haversine Distance (3 decimal places with +-0.001 tolerance) from the closest shopping centre to the property. Default value: 0.\n",
    "14.**Train_station_id**: The closest train station to the property. Default value: 0.\n",
    "15.**Distance_to_train_station**: The Haversine Distance (3 decimal places with +-0.001 tolerance) from the closest train station to the property. Default value: 0.\n",
    "16.**travel_min_to_CBD**: The average travel time (minutes) from the closest train station to the “Flinders street” station on weekdays (i.e. Monday-Friday) departing between 7 to 9 am. For example, if there are 3 trip departing from the closest train station to the Flinders street station on weekdays between 7-9am and each take 6, 7, and 8 minutes respectively, then the value of this column for the property should be (6+7+8)/3. If there are any direct transfers between the closest station and Flinders street station, only the average of direct transfers should be calculated. Default value: 0.\n",
    "17.**Hospital_id**: The closest hospital to the property. Default value: “not available”.\n",
    "18.**Distance_to_hospital**: The Haversine Distance (3 decimal places with +-0.001 tolerance) from the closest hospital to the property. Default value: 0.\n",
    "19.**Supermarket_id**: The closest supermarket to the property.Default value: “not available”.\n",
    "20.**Distance_to_supermaket**: The Haversine Distance (3 decimal places with +-0.001 tolerance) from the closest supermarket to the property. Default value: 0.\n",
    "21.**Over_ave_price**: The Boolean value to the property. Write a True value if the price is higher than the average house price of its suburb. Otherwise, the value should be False. The average house price of each property can be calculated by averaging the total house price of its suburb. Default value: 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "- The libraries which are required for extracting and integrating the data from the given files needs to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import html5lib\n",
    "import lxml\n",
    "import xml.etree.ElementTree as ET\n",
    "import math\n",
    "import xmltodict\n",
    "import PyPDF2\n",
    "from shapely.geometry import shape,mapping, Point, asPoint\n",
    "from tabula.io import read_pdf\n",
    "import shapefile\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting attributes from real_state.json and real_state.xml file\n",
    "- real_state.json and real_state.xml file contains 10 columns which are required.\n",
    "- These both files contain these 10 columns, therefore a union of these files will be taken.\n",
    "- Using pandas, real_state.json file is read which is later stored in 'df'.\n",
    "- Using xmltodict and xm.etree.ElementTree real_state.xml file is read.\n",
    "- Both these files are concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_state.json file is read\n",
    "real_state = pd.read_json(r'real_state.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len of real_state.json\n",
    "len(real_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_state.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking null values in .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_state.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_state.xml is read\n",
    "xmlFile = open(r\"D:\\Jupyter Notebook\\Wrangling\\Assignment 03\\real_state.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while reading \\nb' is stripped\n",
    "xmlData = xmlFile.read().strip(\"\\nb'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml file is parsed\n",
    "root = ET.XML(xmlData) \n",
    "data = []\n",
    "cols = []\n",
    "for i, child in enumerate(root):\n",
    "    data.append([subchild.text for subchild in child])\n",
    "    cols.append(child.tag)\n",
    "\n",
    "xml_data = pd.DataFrame(data).T  # Write in DF and transpose it\n",
    "xml_data.columns = cols  # Update column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking null values in .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len of real_state.xml\n",
    "len(xml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether there is any null in the file\n",
    "xml_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .json and .xml file is concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_real = pd.concat([xml_data,real_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplicates in merge_real, needs to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_real.drop_duplicates(keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The indexes of column are not in order after merge, thus .reset_index() will reset the index of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datatype of property_id from .json and .xml needs to be investigated, because there may be duplicated property_id in the 'df'. And making their datatype same will be helpful in removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df.loc[0, 'property_id']))\n",
    "print(type(df.loc[1015, 'property_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The property_id in .xml file is in string data type.\n",
    "- Thus, need to convert the datatype of property_id to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['property_id'] = pd.to_numeric(df['property_id'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the datatype of property_id is int in 'df', the duplicates can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset='property_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using .duplicated(), I found about the rows which has duplicated values. thus using keep = 'first', I will keep only the first row of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.property_id == 24359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['property_id'], keep='first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again we have checked for duplicates\n",
    "df[df.property_id == 24359]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we don't have any duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In task 01, we have 21 columns. We have extracted 10 columns above. We need to create other columns which will further be extracted, and need to assign default values to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['suburb'] = 'not available'\n",
    "df['Shopping_center_id'] = 'not available'\n",
    "df['Distance_to_sc'] = 0\n",
    "df['Train_station_id'] = 0\n",
    "df['Distance_to_train_station'] = 0\n",
    "df['travel_min_to_CBD'] = 0\n",
    "df['Hospital_id'] = 'not available'\n",
    "df['Distance_to_hospital'] = 0\n",
    "df['Supermarket_id'] = 'not available'\n",
    "df['Distance_to_supermaket'] = 0\n",
    "df['Over_ave_price'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['property_id', 'lat', 'lng', 'addr_street', \n",
    "         'suburb', 'price', 'property_type', 'year', \n",
    "         'bedrooms', 'bathrooms', 'parking_space', \n",
    "         'Shopping_center_id', 'Distance_to_sc',\n",
    "         'Train_station_id', 'Distance_to_train_station',\n",
    "         'travel_min_to_CBD', 'Hospital_id',\n",
    "         'Distance_to_hospital', 'Supermarket_id', 'Distance_to_supermaket', 'Over_ave_price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The numeric datatype of the following columns needs to be assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "df['lng'] = pd.to_numeric(df['lng'], errors='coerce')\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df['bedrooms'] = pd.to_numeric(df['bedrooms'], errors='coerce')\n",
    "df['bathrooms'] = pd.to_numeric(df['bathrooms'], errors='coerce')\n",
    "df['parking_space'] = pd.to_numeric(df['parking_space'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The columns are needed to be renamed as per the requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Property_id', 'lat', 'lng', 'addr_street', \n",
    "         'suburb', 'price', 'property_type', 'year', \n",
    "         'bedrooms', 'bathrooms', 'parking_space', \n",
    "         'Shopping_center_id', 'Distance_to_sc',\n",
    "         'Train_station_id', 'Distance_to_train_station',\n",
    "         'travel_min_to_CBD', 'Hospital_id',\n",
    "         'Distance_to_hospital', 'Supermarket_id', 'Distance_to_supermaket', 'Over_ave_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Index of df needs to reset again, as we removed the duplicates so indices are again non-uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 'index' column needs to be dropped, therefore .drop(columns = [''], inplace =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['index'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Hospital_id, Distance_to_hospital from hospital.xlsx file      \n",
    "- Hospital_id needs to be the closest shopping center to the respective property_id and Distance_to_hospital is the distance from it. It is to be extracted from hospitals.xlsx file.\n",
    "- .xlsx file is read using pd.read_excel, where the unwanted columns have been dropped using .drop().\n",
    "- Using haversine distance formula is used to calculate disctance between two pair of latitude and longitude, therefore created a function ie, haversine_distance.\n",
    "- Another function get_DistId is created to find the nearest the hospital from a property.\n",
    "- Thus, Hospital_id, Distance_to_hospital is calculated using get_DistId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#.xlsx file is read\n",
    "hospital_data = pd.read_excel(\"./hospitals.xlsx\",engine = 'openpyxl').drop(columns = ['name', 'Unnamed: 0']) #https://stackoverflow.com/questions/26063231/read-specific-columns-with-pandas-or-other-python-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of hospital_data\n",
    "hospital_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    radius_of_earth = 6378\n",
    "    #lat1,lon1 = property's coordinates\n",
    "    # lat2, lon2 = coordinates of other location\n",
    "    prop_lat = math.radians(lat1)\n",
    "    prop_long = math.radians(lon1)\n",
    "    another_lat = math.radians(lat2)\n",
    "    another_long = math.radians(lon2)\n",
    "\n",
    "    dlat = prop_lat - another_lat\n",
    "    dlon = prop_long - another_long\n",
    "\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(prop_lat) * math.cos(another_lat) * math.sin(dlon / 2)**2\n",
    "    distance = radius_of_earth * (2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))) \n",
    "    \n",
    "    return round(distance,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_DistId function here takes 4 arguments: df, hospital_data, id, lat, lng.\n",
    "- id, lat, lng are the columns of hospital_data.\n",
    "- In this function, I have used minDistance which has been assigned infinite value. Its been used because any value will be smaller than it. So the smallest of values calculated for each row is saved over minDistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DistId(df1, df2, col_id, lat, lng):\n",
    "    temp = []\n",
    "    for i,j in df1.iterrows():\n",
    "        minDistance = math.inf\n",
    "        temp_id = \"\"\n",
    "        for x,y in df2.iterrows():\n",
    "            distance =  haversine_distance(df1.loc[i,'lat'], df1.loc[i,'lng'],df2.loc[x,lat], df2.loc[x,lng])\n",
    "            if distance < minDistance:\n",
    "                minDistance = distance\n",
    "                temp_id = df2.loc[x,col_id]\n",
    "        temp.append((temp_id, minDistance))\n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing arguments in get_DistId function\n",
    "DistId = get_DistId(df, hospital_data, 'id','lat','lng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values calculated are populated in the Hospital_id and Distance_to_hospital\n",
    "for i, j in df.iterrows():\n",
    "    df.loc[i,'Hospital_id'], df.loc[i,'Distance_to_hospital'] = DistId[i][0], DistId[i][1] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Shopping_center_id, Distance_to_sc from shopingcenters.html \n",
    "- Shopping_center_id needs to be the closest shopping center to the respective property_id and Distance_to_sc is the distance from it. It is to be extracted from shopingcenters.html file.\n",
    "- .html file is read using pd.read_html.\n",
    "- The unwanted columns has been dropped using .drop().\n",
    "- Using get_DistId, the nearest Shopping_center_id and Distance_to_sc has been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file is read\n",
    "shop_cen_data = pd.read_html(\"shopingcenters.html\")[0].drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_cen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of shop_cen_data\n",
    "shop_cen_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_DistId function here takes : df, shop_cen_data, sc_id, lat,lng, where sc_id, lat,lng are the columns of shop_cen_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing arguments in get_DistId function\n",
    "DistId = get_DistId(df, shop_cen_data, 'sc_id', 'lat','lng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values calculated are populated in Shopping_center_id and Distance_to_sc\n",
    "for i, j in df.iterrows():\n",
    "    df.loc[i,'Shopping_center_id'], df.loc[i,'Distance_to_sc'] = DistId[i][0], DistId[i][1] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Supermarket_id, Distance_to_supermaket from supermarkets.pdf\n",
    "- Supermarket_id needs to be the closest supermarket to the respective property_id, and Distance_to_supermaket is the distance to it, which is to be extracted from supermarkets.pdf file.\n",
    "- For finding the number of pages in pdf file provided, PyPDF2 library is imported.\n",
    "- Then tabula library is imported, for reading the file using read_pdf.\n",
    "- Using get_DistId, Supermarket_id, Distance_to_supermaket is calculated, which is nearest to the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file is opened\n",
    "pdfFileObj = open('supermarkets.pdf', 'rb')\n",
    "#file is read using PyPDF2 library\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "#number of pages is read\n",
    "super_mkt_pages=pdfReader.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file is again read using tabula library\n",
    "mkt = read_pdf('supermarkets.pdf', pages = 'all')\n",
    "#an empty dataframe is created\n",
    "super_mkt = pd.DataFrame()\n",
    "#extracting each page in pdf file and appending in created dataframe\n",
    "for i in range(super_mkt_pages):\n",
    "    super_mkt = super_mkt.append(mkt[i])\n",
    "    #index is reset after appending\n",
    "    super_mkt.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unwanted index id dropped\n",
    "super_mkt = super_mkt.drop(columns=['Unnamed: 0', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of super_mkt\n",
    "super_mkt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing arguments in get_DistId function\n",
    "DistId = get_DistId(df, super_mkt, 'id', 'lat', 'lng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values calculated are populated in Supermarket_id and Distance_to_supermarket\n",
    "for i, j in df.iterrows():\n",
    "    df.loc[i,'Supermarket_id'], df.loc[i,'Distance_to_supermaket'] = DistId[i][0], DistId[i][1] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting suburb from vic_suburb_boundary folder.\n",
    "- vic_suburb_boundary folder is provided in .zip folder which has been extracted manually.\n",
    "- This folder contains four files which are required to find the suburb under which a specific property_id falls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapefile is read\n",
    "sf = shapefile.Reader(\"./vic_suburb_boundary/VIC_LOCALITY_POLYGON_shp\")\n",
    "#records of the shapefile\n",
    "recs = sf.records()\n",
    "#points of the polygon in the shape file\n",
    "shapes = sf.shapes() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len of shapes and recs in shapefile\n",
    "len(shapes), len(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of tuples of longitude and latitude of property_id\n",
    "lg_lt = []\n",
    "for i in range(len(df)):\n",
    "    lg_lt.append((df.loc[i,'lng'], df.loc[i,'lat']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an empty list is created to append all the shapes of the suburbs in shapes\n",
    "suburb_shape_list = []\n",
    "for i in shapes:\n",
    "    suburb_shape_list.append(shape(i.__geo_interface__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the suburb of each property and populating it to 'suburb' column in df\n",
    "for i in range(len(lg_lt)):\n",
    "    for j in range(len(suburb_shape_list)):\n",
    "        if suburb_shape_list[j].contains(Point(lg_lt[i])):\n",
    "            df.loc[i,'suburb'] = recs[j][6]\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Over_ave_price with price and suburb columns.\n",
    "- Over_ave_price includes boolean values.\n",
    "- In Over_ave_price, if the value of a property is higher than the average property of its suburb then True needs to added to the column or else False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function created to calculate the average values of each suburb\n",
    "def suburb_average(sub):\n",
    "    cost = df.loc[df['suburb'] == sub, 'price']\n",
    "    total = 0\n",
    "    for i in cost:\n",
    "        total += i\n",
    "    \n",
    "    return total / len(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A dictionary is created which will store the average value of each suburb in df. This will help in not calculating the average price of each suburb of every row in 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_dict = {}\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i,'suburb'] not in average_dict:\n",
    "        average_dict[df.loc[i,'suburb']] = suburb_average(df.loc[i,'suburb'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values populated in Over_ave_price column\n",
    "for i in range(len(df)):\n",
    "    df.loc[i,'Over_ave_price'] = df.loc[i,'price'] > average_dict[df.loc[i, 'suburb']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Train_station_id and Distance_to_train_station from  1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015) folder.\n",
    "- Train_station_id needs to be the closest train station to the respective property_id, and Distance_to_train_station is the distance to it, which is to be extracted from stops.txt file in 1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015) folder.\n",
    "- The folder provided was in .zip format, whose files have been extracted manually.\n",
    "- The folder contains eight .txt file.\n",
    "- Among these eight files, stops.txt file is used to extract the Train_station_id and Distance_to_train_station, which are nearest to property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a text file is read\n",
    "stops_data = pd.read_csv(r'./1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)/GTFS - Melbourne Train Information/stops.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unwanted columns are dropped\n",
    "stops_data = stops_data.drop(columns = ['stop_short_name', 'stop_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of stops.txt file\n",
    "stops_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing arguments in get_DistId function\n",
    "DistId = get_DistId(df, stops_data, 'stop_id', 'stop_lat', 'stop_lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values populated are populated in Train_station_id and Distance_to_train_station\n",
    "for i, j in df.iterrows():\n",
    "    df.loc[i,'Train_station_id'], df.loc[i,'Distance_to_train_station'] = DistId[i][0], DistId[i][1] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting travel_min_to_CBD from GTFS - Melbourne Train Information - From PTV (9 Oct 2015) folder.\n",
    "- travel_min_to_CBD is the time taken from the respective Train_station_id to the 'Flinders Street' on weekdays (Mon-Fri) between 7 am -9am.\n",
    "- For this column, the average value needs to be calculated for the closest train station to Flinders Street.\n",
    "- To calculate the value of travel_min_to_CBD, calender.txt, routes.txt, trips.txt, stops.txt, stops_time.txt needs to be used which are inside GTFS - Melbourne Train Information - From PTV (9 Oct 2015) zip folder which has been extracted manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files required are read\n",
    "calender_data = pd.read_csv(r'D:\\Jupyter Notebook\\Wrangling\\Assignment 03\\1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)\\GTFS - Melbourne Train Information\\calendar.txt')\n",
    "routes_data = pd.read_csv(r'D:\\Jupyter Notebook\\Wrangling\\Assignment 03\\1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)\\GTFS - Melbourne Train Information\\routes.txt')\n",
    "trips_data = pd.read_csv(r'D:\\Jupyter Notebook\\Wrangling\\Assignment 03\\1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)\\GTFS - Melbourne Train Information\\trips.txt')\n",
    "stop_times_data = pd.read_csv(r'D:\\Jupyter Notebook\\Wrangling\\Assignment 03\\1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)\\GTFS - Melbourne Train Information\\stop_times.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For travel_min_to_CBD column, service_id needs to be found out which run from Monday to Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calender_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_service = calender_data.loc[(calender_data['monday'] == 1) & (calender_data['tuesday'] == 1) & (calender_data['wednesday'] == 1) & (calender_data['thursday'] == 1) & (calender_data['friday'] == 1), 'service_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_service.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the stop_id fro Flinders Street needs to to find from stops_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_id = stops_data[stops_data[\"stop_name\"].str.contains(\"Flinders Street\")].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_id #stop_id for flinders street"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now I found the route_id from routes_data which run from another station to 'Flinders Station'. Therefore, I have splitted route_long_name with respect to \"-\" and its [1] index is appended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = pd.DataFrame()\n",
    "for i,j in routes_data.iterrows():\n",
    "    x = j.route_long_name.split(\"-\")\n",
    "    if x[1] == ' City (Flinders Street)':\n",
    "        fs = fs.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it includes table which goes to Flinders Street\n",
    "fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In trips_data, it includes the route_id, service_id which has been extracted above.In this dataframe, direction_id is '0' when it goes towards the Flinders Street and '1' when it come from Flinders Street.\n",
    "- Thus, I extracted the route_id which goes towards flinders street (extracted above), and service_id = 'T0' (extracted above) and direction_id = '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_trip = trips_data[(trips_data['service_id'] == 'T0') & (trips_data['route_id'].isin(fs.route_id)) & (trips_data['direction_id'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_trip.shape #shape of fs_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In stop_times_data, need to extract the trip_id from the condition defined above, departure_time and arrival_time between 7am - 9am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_stop = stop_times_data[stop_times_data['trip_id'].isin(fs_trip.trip_id) & (stop_times_data['departure_time'] >= '07:00:00') & (stop_times_data['arrival_time'] <= '09:00:00')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have created a column to store the timetaken to reach Flinders Street from a particular Train_station_id, and removed the columns which are of no use right now. Also the index has been resetted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_stop['time_taken'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_stop = fs_stop.drop(columns=[\"shape_dist_traveled\", \"stop_headsign\", \"pickup_type\", \"drop_off_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_stop = fs_stop.reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have calculated the time taken to flinders street in opposite direction. If we go from one station to flinders street or come back to this station, time taken must be same.\n",
    "- I have started calculating time when flinders street come across to every station above it, and that time has been added in every row as I move from down to above. This method will save the time as its time complexity will be low. Also I have keep the trip-id in check while calculating time. Thats why i have created a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripId = \"\"\n",
    "isFs = False\n",
    "tformat = \"%H:%M:%S\"\n",
    "for i in range(len(fs_stop) - 1, 0, -1):\n",
    "    #   \n",
    "    if tripId != fs_stop.loc[i, 'trip_id']:\n",
    "        isFs = False\n",
    "    \n",
    "    if fs_stop.loc[i, 'stop_id'] == 19854:\n",
    "        isFs = True\n",
    "        tripId = fs_stop.loc[i, 'trip_id']\n",
    "        \n",
    "    elif isFs:\n",
    "        s1 = fs_stop.loc[i, \"arrival_time\"]\n",
    "        s2 = fs_stop.loc[(i + 1), \"arrival_time\"]\n",
    "        sdelta = datetime.strptime(s2, tformat) - datetime.strptime(s1, tformat)\n",
    "        fs_stop.loc[i, 'time_taken'] = fs_stop.loc[(i + 1), \"time_taken\"] + sdelta.seconds\n",
    "    \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last 20 rows of fs_stop\n",
    "fs_stop.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A function is created to iterate through rows in 'df' to find the Train_station_id of each row, which will take 2 arguments: stop_id of station that are stored in 'df' as Train_station_id and 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_taken(stopId, df):\n",
    "    temp = df[(df[\"stop_id\"] == stopId) & (df[\"time_taken\"] != 0)]\n",
    "    total = 0\n",
    "    for i, j in temp.iterrows():\n",
    "        total += j.time_taken\n",
    "    \n",
    "    if len(temp) != 0:\n",
    "        return (total / len(temp)) // 60\n",
    "    else: return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterated in df\n",
    "for i,j in df.iterrows():\n",
    "     df.loc[i,'travel_min_to_CBD'] = time_taken(df.loc[i, 'Train_station_id'], fs_stop)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have created a .csv for the information extracted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"31901611_A3_solution.csv\",  index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For defining data integration.\n",
    " [https://www.talend.com/resources/what-is-data-integration/]\n",
    "- For understanding xml.etree.ElementTree library.\n",
    " [https://docs.python.org/3/library/xml.etree.elementtree.html]\n",
    "- For extracting the data of xml file.\n",
    " [https://medium.com/@robertopreste/from-xml-to-pandas-dataframes-9292980b1c1c]\n",
    "- For renaming the columns.\n",
    " [https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas]\n",
    "- For understanding and using tabula library.\n",
    " [https://tabula-py.readthedocs.io/en/latest/tabula.html]\n",
    "- For solving a doubt over extraction of values in file.\n",
    " [https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe]\n",
    "- For mapping table.\n",
    " [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html]\n",
    " [https://kanoki.org/2020/01/21/pandas-dataframe-filter-with-multiple-conditions/]\n",
    "- For opening the shapefile.\n",
    " [https://pythonhosted.org/Python%20Shapefile%20Library/]\n",
    "- For calculating time difference between two time strings.\n",
    " [https://stackoverflow.com/questions/3096953/how-to-calculate-the-time-interval-between-two-time-strings]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
